# Projeto IA - Reconhecimento de emo√ß√µes

<aside>
üí° A ideia principal do projeto √© utilizar uma API da Google (Cloud Vision), para extrair emo√ß√µes do usu√°rio, pela webcam.

</aside>

Na configura√ß√£o proposta, os equipamentos principais s√£o: um bot√£o, um monitor, uma Raspberry Pi e uma webcam. O procedimento se inicia com o usu√°rio posicionando seu rosto diante da webcam para que seja exibido na tela do monitor. Em seguida, o bot√£o √© pressionado para acionar o processo de captura de imagem. Automaticamente, a foto √© tirada e os dados faciais s√£o enviados para uma API por meio de uma requisi√ß√£o. Ap√≥s um curto per√≠odo, a foto capturada √© apresentada na tela juntamente com um ret√¢ngulo que delimita o rosto do usu√°rio, acompanhado de uma legenda que descreve a emo√ß√£o identificada.


## `Esquema visual - Brainstorming:`

![Brainstorming](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/65148cd1-6bc1-43fa-b73c-59895b9457a3)

## `GCP:`

- Cloud Vision;
- Credenciais.

## `RASP:`

- Request ‚Üí API GCP;
- JSON ‚Üí Chaves.

## `Est√©tica visual:`

Planejamento de uma est√©tica visual visando o relacionamento entre o usu√°rio, o monitor, e o bot√£o que ser√° usado para iniciar o processo de captura.

## **`Equipamentos:`**

- Raspberrry Pi 4;
- Monitor;
- Webcam.

## `Software:`

- Instala√ß√£o do VSCODE;
- Instala√ß√£o da Rasp OS Debian;
- Instala√ß√£o do GIT.

## `Hardware:`

- Bot√£o ‚Üí Resistor ‚Üí Pull-Down.

## Raspberry

‚Üí Link para o site oficial: [Raspberry Pi](https://www.raspberrypi.com/)

### O que √© a Raspberry?

![Raspberry](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/b9b85bec-d499-4b02-9375-d6b8365db440)

O Raspberry Pi (RPi) √© um microcomputador de placa √∫nica projetado pela Raspberry Pi Foundation, lan√ßado inicialmente em 2012 com o modelo Raspberry Pi 1 Model B. Ele √© uma alternativa acess√≠vel e compacta aos computadores tradicionais, contendo todos os componentes essenciais em uma √∫nica placa.

Equipado com processador, mem√≥ria RAM, placa de v√≠deo integrada e diversos conectores (como USB, HDMI, √°udio e GPIO), o Raspberry Pi suporta sistemas operacionais baseados em Linux e pode ser utilizado para uma variedade de finalidades. Desde atividades educacionais em programa√ß√£o e eletr√¥nica at√© projetos de automa√ß√£o residencial, servidores de m√≠dia, IoT e prototipagem.

### Procedimento para utiliza√ß√£o da Raspberry

**‚Üí Download da imagem da Raspberry no cart√£o SD**

![Download Rasp](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/43c0610e-1f53-4f49-a8e4-995e1e0473f7)

**‚Üí Configurar Raspberry Pi Imager**

![Config rasp](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/29791fca-ad97-4d61-bffe-ee43250a3af7)

- **Raspberry Pi Device:** Raspberry Pi 4;
- **Operating System:** Raspberry Pi OS (64-bit);
- **Storage:** Generic- SD/MMC/MS PRO USB Device-31.9GB.

**‚Üí NEXT**

![Customisation Rasp](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/8344d348-7e68-4a7c-832f-895cf0a83e82)

**‚Üí EDIT SETTINGS**

‚Üí GENERAL:

![Settings rasp](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/a07bbbdd-b534-4ff2-beec-7f1f6c07fe0d)

Configura√ß√µes:

- **Set hostname:** projetoIA;
- **Username:** instrutor;
- **Set locale settings**
- **Time zone:** America/Sao_Paulo;
- **Keyboard layout:** pt.

‚Üí SAVE

‚Üí SERVICES:

![Services rasp](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/2c7861d8-8ddd-413a-b28b-e59ce0b62115)

Enable SSH ‚úÖ

SSH - √â um protocolo de comunica√ß√£o remoto via terminal, pelo terminal conseguimos acessar a RASP e realizar configura√ß√µes.

‚Üí SAVE

![SAVE RASP](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/8e28c164-d4c5-4573-8ebd-8418f36f6557)

‚Üí Clicar no bot√£o ‚ÄúYES‚Äù para a cria√ß√£o da imagem no cart√£o SD.

## Python

‚Üí Link para o site oficial: [Python](https://www.python.org/)

### Prepara√ß√£o do ambiente - Python:

‚Üí Instala√ß√£o e Configura√ß√£o do VScode

Obs: A instala√ß√£o do VScode foi realizada somente para o desenvolvimento do projeto. Ap√≥s a aprova√ß√£o da interface, a utiliza√ß√£o do python passou a ser utilizada pelo terminal, para melhorar a otimiza√ß√£o.

*Antes da instala√ß√£o do VScode, a pr√≥pria Rasp d√° uma op√ß√£o para a atualiza√ß√£o dos sistema operacional.

‚Üí No terminal:

```python
pip install code
```

```python
code
```

‚Üí No VScode, realizar a instala√ß√£o e ativa√ß√£o do python (extens√µes): 

![Extensoes Py](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/6926ddcb-4f7e-4faf-9f1c-7b189e5482e0)

---

### `Configura√ß√£o do ambiente sem o VScode`

‚Üí Prepara√ß√£o do ambiente pelo terminal:

- Atualiza√ß√£o dos pacotes da RASP

```jsx
sudo apt-get update
sudo apt-get upgrade
```

- Cria√ß√£o de pasta para o projeto

```jsx
MKDIR ProjetoIA
```

- Clonar o reposit√≥rio do git

```jsx
git clone https://github.com/ccadu86/IA-EMO-ES.git
```

- Cria√ß√£o de ambiente virtual

```jsx
python -m venv venv
```

- Ativar ambiente virtual

```jsx
source venv/bin/activate
```

- Instala√ß√µes das bibliotecas

```jsx
pip install opencv-python
pip install --upgrade google-cloud-vision
pip install RPi-GPIO
```

- Start aplica√ß√£o

```jsx
python main.py
```

## Cloud Vision API

**Documenta√ß√£o do Cloud Vision**

[Documenta√ß√£o do Cloud Vision ¬†|¬† Cloud Vision API ¬†|¬† Google Cloud](https://cloud.google.com/vision/docs?hl=pt-br)

- ACESSAR GOOGLE CLOUD PLATFORM (GCP):

[Cloud Computing Services | Google Cloud](https://cloud.google.com/)

1. Resgate dos cr√©ditos (US$300) para o projeto
2. Acesso ao produto: **Cloud Vision API**

---

- Ao realizar o acesso √† plataforma, h√° um projeto pr√©-criado pela plataforma. Por√©m, foi optado a cria√ß√£o de um novo projeto.

**Nome do Projeto**: IA - EMOCOES

- Utiliza√ß√£o do **Face Detection**, funcionalidade dispon√≠vel na IA do Cloud Vision.
- Ativa√ß√£o da API do Cloud Vision pela plataforma.

---

Detalhes do Produto - Cloud Vision API

**Valor para utiliza√ß√£o da API - FACE DETECTIONS OPERATIONS:**

| REQUISI√á√ïES | 0 at√© < 1000/m√™s | > 1000/m√™s |
| --- | --- | --- |
| BRL | 0,00 | 8,870629612 |
- Reconhece a emo√ß√£o a partir da captura de imagem.
- Ao realizar a habilita√ß√£o da API √© necess√°rio uma credencial de API para realizar acessos.
- Cria√ß√£o de uma **Conta de Servi√ßo: IA_EMOCOES**. Ao criar uma ****conta de servi√ßo √© poss√≠vel utilizar qualquer servi√ßo do Google Cloud Platform.
- Done
- Acesso √† conta
- Acesso √†s Chaves e gerar arquivo JSON (***credentials.json***)

## Integra√ß√£o com a Webcam

‚Üí Realiza√ß√£o dos primeiros testes da integra√ß√£o da WebCam.

**Ideia Principal**

Na Raspberry √© feito um Request na API do Cloud Vision, que nos disponibilizar√° um JSON com as informa√ß√µes geradas a partir da requisi√ß√£o.

Dentro do JSON deve ser procurado a chave onde √© informada as emo√ß√µes que foram encontradas.

*Tarefa realizada ap√≥s a configura√ß√£o da API.

- No terminal, instalar a biblioteca do Google com o seguinte comando:

```html
pip install --upgrade google-cloud-vision
```

### `Teste 1:`

- Cria√ß√£o do arquivo def_imports.py

```python
from google.cloud import vision

def detect_faces(path):
    """Detecta rostos em uma imagem."""

    # Cria um cliente para o servi√ßo de anota√ß√£o de imagens do Google Cloud Vision
    client = vision.ImageAnnotatorClient()

    # Abre o arquivo de imagem no caminho especificado e l√™ seu conte√∫do
    with open(path, "rb") as image_file:
        content = image_file.read()

    # Cria um objeto de imagem a partir do conte√∫do lido
    image = vision.Image(content=content)

    # Solicita a detec√ß√£o de rostos na imagem
    response = client.face_detection(image=image)
    faces = response.face_annotations

    # Nomes das probabilidades de emo√ß√µes, conforme definido no google.cloud.vision.enums
    likelihood_name = (
        "UNKNOWN",         # Desconhecido
        "VERY_UNLIKELY",   # Muito Improv√°vel
        "UNLIKELY",        # Improv√°vel
        "POSSIBLE",        # Poss√≠vel
        "LIKELY",          # Prov√°vel
        "VERY_LIKELY",     # Muito Prov√°vel
    )
    print("Faces:")

    # Itera sobre cada rosto detectado na imagem
    for face in faces:
        # Imprime a probabilidade de cada emo√ß√£o para o rosto detectado
        print(f"anger: {likelihood_name[face.anger_likelihood]}")      # Raiva
        print(f"joy: {likelihood_name[face.joy_likelihood]}")          # Alegria
        print(f"surprise: {likelihood_name[face.surprise_likelihood]}") # Surpresa

        # Obt√©m as coordenadas dos v√©rtices do pol√≠gono delimitador do rosto
        vertices = [
            f"({vertex.x},{vertex.y})" for vertex in face.bounding_poly.vertices
        ]

        # Imprime as coordenadas dos v√©rtices do rosto
        print("face bounds: {}".format(",".join(vertices)))

    # Verifica se houve algum erro na resposta
    if response.error.message:
        # Levanta uma exce√ß√£o com a mensagem de erro e um link para mais informa√ß√µes
        raise Exception(
            "{}\nFor more info on error messages, check: "
            "https://cloud.google.com/apis/design/errors".format(response.error.message)
        )
```

- Cria√ß√£o do arquivo main.py

```python
from def_imports import *
import os

os.environ ["GOOGLE_APPLICATION_CREDENTIALS"] = 'credentials.json'

print(detect_faces('foto.jpeg'))
```

- Resultado no terminal ap√≥s o teste:

```python
(.venv) instrutor@proetoIA:~/Documents/IA-EMO-ES $ python main.py
Faces:
anger: VERY_UNLINKELY
joy: VERY_LIKELY
surprise: VERY_UNLIKELY
face nounds: (246,67), (775,67), (775,683), (246,683)
None
```

### `Teste 2:`

- Arquivo def_imports.py

```python
from google.cloud import vision

def detect_faces(path):
    """Detecta rostos em uma imagem."""

    # Cria um cliente para o servi√ßo de anota√ß√£o de imagens do Google Cloud Vision
    client = vision.ImageAnnotatorClient()

    # Abre o arquivo de imagem no caminho especificado e l√™ seu conte√∫do
    with open(path, "rb") as image_file:
        content = image_file.read()

    # Cria um objeto de imagem a partir do conte√∫do lido
    image = vision.Image(content=content)

    # Solicita a detec√ß√£o de rostos na imagem
    response = client.face_detection(image=image)
    faces = response.face_annotations

    # Nomes das probabilidades de emo√ß√µes, conforme definido no google.cloud.vision.enums
    likelihood_name = (
        "UNKNOWN",         # Desconhecido
        "VERY_UNLIKELY",   # Muito Improv√°vel
        "UNLIKELY",        # Improv√°vel
        "POSSIBLE",        # Poss√≠vel
        "LIKELY",          # Prov√°vel
        "VERY_LIKELY",     # Muito Prov√°vel
    )
    print("Faces:")

#Comentar essa parte
    # Itera sobre cada rosto detectado na imagem
    #for face in faces:
        # Imprime a probabilidade de cada emo√ß√£o para o rosto detectado
        #print(f"anger: {likelihood_name[face.anger_likelihood]}")      # Raiva
        #print(f"joy: {likelihood_name[face.joy_likelihood]}")          # Alegria
        #print(f"surprise: {likelihood_name[face.surprise_likelihood]}") # Surpresa

        # Obt√©m as coordenadas dos v√©rtices do pol√≠gono delimitador do rosto
        #vertices = [
            #f"({vertex.x},{vertex.y})" for vertex in face.bounding_poly.vertices
        #]

        # Imprime as coordenadas dos v√©rtices do rosto
        #print("face bounds: {}".format(",".join(vertices)))

    # Verifica se houve algum erro na resposta
    if response.error.message:
        # Levanta uma exce√ß√£o com a mensagem de erro e um link para mais informa√ß√µes
        raise Exception(
            "{}\nFor more info on error messages, check: "
            "https://cloud.google.com/apis/design/errors".format(response.error.message)
        )
```

- Arquivo main.py

```python
from def_imports import *
import os

os.environ ["GOOGLE_APPLICATION_CREDENTIALS"] = 'credentials.json'

print(detect_faces('foto.jpeg'))
```

- Resultado no terminal ap√≥s o teste:

```python
Faces: [bounding_poly {
	vertices {
		x: 246
		y: 67
	}
	vertices {
		x: 775
		y: 67
	}
	vertices {
		x: 775
		y: 683
	}
	vertices {
		x: 246
		y: 683 
	}
}
fd_bounding_poly {
	vertices {
		x: 301
		y: 220
	}
	vertices {
		x: 718
		y: 220
	}
	vertices {
		x: 718
		y: 637
	}
	vertices {
		x: 301
		y: 637 
	} 
} 
landmarks {
	type_:
		LEFT_EYE position {
			x: 434.759918
			y: 365.621918
			z: -0.00130653381
		}
	}
	landmarks {
		type_: RIGHT_EYE position {
			x: 594.609863
			y: 365.956329
			z: 3.84433651
		}
	}
	landmarks {
		type_: LEFT_OF_LEFT_EYEBROW position {
			x: 380.010223
			y: 323.234192
			z: 14.1111031
		}
	}
	landmarks {
		type_: RIGHT_OF_LEFT_EYEBROW position {
			x: 478.717926
			y: 319.387024
			z: -26.6964569
		}
	} 
	landmarks {
		type_: LEFT_OF_RIGHT_EYEBROW position {
			x: 550.675232
			y: 320.928833
			z: -25.0222549
		}
	}
	landmarks {
		type_: RIGHT_OF_RIGHT_EYEBROW position {
			x: 648.830078
			y: 323.236908
			z: 20.5603962
		}
	}
	landmarks {
		type_: MIDPOINT_BETWEEN_EYES position {
			x: 514.295654
			y: 359.641
			z: -29.4551201
		}
	}
	landmarks {
		type_: NOSE_TIP position {
			x: 515.89563
			y: 451.595
			z: -77.2197418
		}
	}
	landmarks {
		type_: UPPER_LIP position {
			x: 516.562256
			y: 504.816071
			z: -43.5845261
		}
	}
	landmarks {
		type_: LOWER_LIP position {
			x: 515.569275
			y: 563.684875
			z: -34.6525879
		}
	}
	landmarks {
		type_: MOUTH_LEFT position {
			x: 429.596313
			y: 518.402222
			z: -2.25559235
		}
	}
	landmarks {
		type_: MOUTH_RIGHT position {
			x: 595.857117
			y: 520.15271
			z: 0.89873457
		}
	}
	landmarks {
		type_: MOUTH_CENTER position {
			x: 515.725708
			y: 532.28772
			z: -33.1510773
		}
	}
	landmarks {
		type_: NOSE_BOTTOM_RIGHT position {
			x: 566.228821
			y: 465.132111
			z: -19.8852158
		}
	}
	landmarks {
		type_: NOSE_BOTTOM_LEFT position {
			x: 463.280396
			y: 463.98996
			z: -21.9731503
		}
	}
	landmarks {
		type_: NOSE_BOTTOM_CENTER position {
			x: 516.443359
			y: 479.795929
			z: -43.8077888
		}
	} landmarks {
		type_: LEFT_EYE_TOP_BOUNDARY position {
			x: 434.96814
			y: 350.403656 
			z: -8.28114414
		}
	}
	landmarks {
		type_: LEFT_EYE_RIGHT_CORNER position {
			x: 469.078186
			y: 367.866638
			z: 0.688108444
		}
	}
	landmarks {
		type_: LEFT_EYE_BOTTOM_BOUNDARY position {
			x: 434.007385 
			y: 377.524109 
			z: -1.99274445
		}
	}
	landmarks {
		type_: LEFT_EYE_LEFT_CORNER position {
			x: 400.206451 
			y: 366.176056 
			z: 13.1190462 
		}
	}
	landmarks {
		type_: RIGHT_EYE_TOP_BOUNDARY position {
			x: 595.895569 
			y: 352.349976 
			z: -4.51774073
		}
	}
	landmarks {
		type_: RIGHT_EYE_RIGHT_CORNER position { 
			x: 627.879883 
			y: 368.318695 
			z: 18.5005665
		}
	}
	landmarks {
		type_: RIGHT_EYE_BOTTOM_BOUNDARY position {
			x: 595.238098 
			y: 377.830048 
			z: 1.95094728
		}
	} 
	landmarks {
		type_: RIGHT_EYE_LEFT_CORNER position {
			x: 563.714844 
			y: 367.412201 
			z: 2.98846149
		}
	}
	landmarks { 
		type_: LEFT_EYEBROW_UPPER_MIDPOINT position {
			x: 428.018341 
			y: 306.688873 
			z: -15.4812593
		}
	} 
	landmarks { 
		type_: RIGHT_EYEBROW_UPPER_MIDPOINT position { 
			x: 600.341064 
			y: 307.378357 
			z: -11.4225082
		}
	} 
	landmarks { 
		type_: LEFT_EAR_TRAGION position { 
			x: 326.388641 
			y: 418.985199 
			z: 187.66629
		}
	}
	landmarks {
		type_: RIGHT_EAR_TRAGION position {
			x: 693.065 
			y: 419.866425 
			z: 196.387131
		}
	} 
	landmarks {
		type_: FOREHEAD_GLABELLA position { 
			x: 512.72876 
			y: 323.332184 
			z: -31.4045334
		}
	}
	landmarks {
		type_: CHIN_GNATHION position { 
			x: 517.847595 
			y: 640.319214 
			z: -15.0868464
		}
	} 
	landmarks { 
		type_: CHIN_LEFT_GONION position {
			x: 353.873596 
			y: 549.23877 
			z: 119.562744
		}
	}
	landmarks {
		type_: CHIN_RIGHT_GONION position {
			x: 660.976318 
			y: 554.016 
			z: 127.350922
		}
	}
	landmarks {
		type_: LEFT_CHEEK_CENTER position {
			x: 398.043488 
			y: 464.291412 
			z: 11.6552858 
		}
	}
	landmarks {
		type_: RIGHT_CHEEK_CENTER position {
			x: 630.060547 
			y: 466.530945 
			z: 16.9067898
		}
	}
	roll_angle: -0.0073674405
	pan_angle: 1.40242183 
	tilt_angle: 0.687861621 
	detection_confidence: 0.98046875 
	landmarking_confidence: 0.723549843 
	joy_likelihood: VERY_LIKELY 
	sorrow_likelihood: VERY_UNLIKELY 
	anger_likelihood: VERY_UNLIKELY 
	surprise_likelihood: VERY_UNLIKELY 
	under_exposed_likelihood: VERY_UNLIKELY 
	blurred_likelihood: VERY_UNLIKELY 
	headwear_likelihood: VERY_LIKELY 
]

## Configura√ß√£o do Hardware

![a.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/df22d4e2-0408-43fc-b139-e6be3d34d022/e7520a8c-3d27-4aec-bcda-e08b6c7b8ad5/a.png)

### Sistema de Pull Down da Raspberry Pi 4

Utilizamos um circuito com resistores de pull-down integrados ao pino GPIO17 (General Purpose Input/Output). Esse resistor √© utilizado para garantir que os pinos GPIO tenham um estado definido (alto ou baixo) quando n√£o est√° sendo ativamente acionado por um dispositivo externo, no caso o bot√£o presente no nosso projeto.

**Resistores de Pull Down**

- **Pull Down**: Um resistor de pull-down √© conectado entre o pino GPIO e o terra (GND). Assegurando que o pino esteja em um estado de baixo (0V) quando nenhum sinal √© aplicado. Sem o resistor de pull-down, o pino ficaria flutuante e poderia ler valores imprevis√≠veis devido a interfer√™ncias el√©tricas.

**Configura√ß√£o**

Na Raspberry Pi, os resistores de pull-down podem ser configurados programaticamente usando a biblioteca GPIO (por exemplo, RPi.GPIO no Python).

```jsx
import RPi.GPIO as GPIO

# Configura o modo da pinagem
GPIO.setmode(GPIO.BCM)

# Define o pino 17 (GPIO 17) como entrada com pull-down
GPIO.setup(17, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)
```

### Utiliza√ß√£o de Portas e Fios no Pinout da Raspberry Pi 4

- **Porta GROUND** (9): FIO MARROM
    - **Fun√ß√£o**: Conex√£o ao terra (GND) da Raspberry Pi. Isso fornece um referencial de 0V para os circuitos.
    
- **Porta GPIO 17** (11): FIO AMARELO
    - **Fun√ß√£o:** Sensor que realiza o envio de sinal. Configurado como uma entrada com resistor de pull-down.
    
- **Porta GPIO 27** (13): FIO LARANJA (reserva)
    - **Fun√ß√£o**: Outro pino GPIO configur√°vel. Reservado para futuras conex√µes ou utiliza√ß√µes.

- **Porta 3V3 power** (17): FIO VERMELHO
    - **Fun√ß√£o**: Fornece uma sa√≠da de 3.3V da Raspberry Pi. √â utilizado como alimenta√ß√£o de sensores ou outros componentes eletr√¥nicos que operam com 3.3V.

Diagrama de pinagem da Raspberry Pi 4:
![diagrama](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/955c4a51-f12b-48f7-84f9-a323395c176a)

![rasp](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/8a6688f0-d393-4010-b7e9-74b30a5f2755)

**Componente bot√£o**

- Realiza o pull-down na opera√ß√£o.

![Bot√£o](https://github.com/ccadu86/IA-EMO-ES/assets/134337212/a34fc537-d9c5-4bbb-a711-4d461bc52393)

```

